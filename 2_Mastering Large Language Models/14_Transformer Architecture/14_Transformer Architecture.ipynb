{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5e1d5e-4837-40ce-9fb8-8b20b729d6e9",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "PPT: https://docs.google.com/presentation/d/1UKEVAWmYrIeaS4ZU2TciEhYxSz4JqS3xnfcQ69ilp-A/edit?usp=sharing\n",
    "\n",
    "Fine web dataset: https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1\n",
    "\n",
    "Tokenizer: https://tiktokenizer.vercel.app/\n",
    "\n",
    "Comparison of LLMs: https://docs.google.com/spreadsheets/d/1S3QfdO4b7Lurucy5FSZQ18coKG3MjzyCd5_HPY-RVbs/edit?usp=sharing\n",
    "\n",
    "Word to Vector representation: https://newsletter.himanshuramchandani.co/p/ai-looking-human-words-ft-stanford\n",
    "\n",
    "Attention is all You need research paper: https://arxiv.org/pdf/1706.03762\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d921f9-9b59-4a6b-93d1-8c3cc004eae3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bc3ce-de9c-441e-b57a-57313f805036",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a story.\n",
    "Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceecfcd-afec-452e-a86c-a888f38b4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a story. \n",
    "Once upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42591fa5-9ec2-431b-b5d6-78d6e10069d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a story. \n",
    "Once upon a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51bd56-346c-4deb-ba7b-fe7548bd8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a story. \n",
    "Once upon a time ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969887ba-fa7b-468a-9807-dc7e87748882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23240c-2d50-4d4e-8f56-f5037a853d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Types of LLM architecture\n",
    "\n",
    "- Decoder-only (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0aac4-9c31-4c26-aaa7-19c9c33871d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer\n",
    "\n",
    "- encoder - process the input to create contextual representation\n",
    "- decoder - generates output (text generation)\n",
    "- self-attention\n",
    "- feed-forward nn\n",
    "- positional encoding\n",
    "- layer normalization and residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02642e3-88f6-4cb7-921a-779c5703f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The cat chased the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f4fd5-50c6-4dee-a9d4-d784507d2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Tokenization\n",
    "\n",
    "[\"The\", \"cat\", \"chased\", \"the\", \"dog\"]\n",
    "\n",
    "assign token IDs\n",
    "\n",
    "\"The\" - ID 100\n",
    "\"cat\" - ID 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6def59-4729-43e7-9dc8-b22f11ce3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Word Embedding\n",
    "\n",
    "token is converted into a vector (embedding) that captures its meaning\n",
    "\n",
    "vector dimension of 512\n",
    "\n",
    "\"cat\" - ID 200 = [0.1, -0.3, 0.7 .... 0.5]\n",
    "\n",
    "output of word embedding - (5,512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a89cf0-c2b4-49b6-a9cf-40135fc6d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Positional encoding\n",
    "\n",
    "adding information about the position of the token in the sentence\n",
    "\n",
    "[\"The\"(position 0), \"cat\"(position 1), \"chased\"(position 2), \"the\"(position 3), \"dog\"(position 4)]\n",
    "\n",
    "\n",
    "output - (5,512) embedding + positional encoding\n",
    "\n",
    "\n",
    "example\n",
    "- The cat chased the dog.\n",
    "- The dog chased the cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfe67b-3053-474d-b5ab-aebed8135e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Encoder input\n",
    "\n",
    "encoder contains N identical layers\n",
    "each layer contains 2 sub components\n",
    "- multi-head self attention\n",
    "- feed-forward neural network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bb112-1dc2-4c5f-a852-678251b82782",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. mutli-head self attention\n",
    "\n",
    "relationship between all the tokens in the sentence\n",
    "\n",
    "also have weights to most important words in the sentence\n",
    "\n",
    "output - (5,512) with attention weights to most relevant words with respect to each other\n",
    "\n",
    "example - cat and chased have high weights\n",
    "\n",
    "\n",
    "capture the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c601c-c691-4029-ac37-f8fd403fb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. residual connection and layer normalization\n",
    "\n",
    "attention([\"The\", \"cat\", \"chased\", \"the\", \"dog\"]) + [\"The\", \"cat\", \"chased\", \"the\", \"dog\"]\n",
    "\n",
    "\n",
    "to preserve the information and stabilize training\n",
    "\n",
    "\n",
    "output - (5,512) with contextualized token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5727cd-5258-43ad-893e-3b002267f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. feed-forward neural network\n",
    "\n",
    "FFN (x) = ReLU(xW1 + b1)W2 + b2\n",
    "\n",
    "\n",
    "output - (5,512) with transformed token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df2bdc-7bb9-4e0b-b657-4b493edcb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. encoder layer\n",
    "\n",
    "output - final matrix (5,512) where each tokens representation is highly contextualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa74d16-abe7-45b3-9a37-eee8aeae2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. output representation\n",
    "\n",
    "ready to used in any task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f02ead-1718-43df-96cd-72034b4ecf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
